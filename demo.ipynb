{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NNのデモコード"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### いくつかエラーが出るようにしているので、調べながら修正してみてください。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### データセットをモデルに入れるための準備"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "必要なもののインポート"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import torch.utils.data as data\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### データリスト関数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def makeDataList(csv_path):\n",
    "    datalist = pd.read_csv(csv_path)\n",
    "    datalist = datalist.drop([\"Name\", \"Ticket\", \"Cabin\"], axis=1)\n",
    "    datalist = pd.get_dummies(datalist)\n",
    "    datalist = datalist.fillna(-1)\n",
    "    return datalist\n",
    "datalist = makeDataList(\"/workspace/dataset/train.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "確認"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"datalist.values[0] =\", datalist.values[0])\n",
    "datalist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### データ分割"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_datalist, val_datalist = train_test_split(datalist, test_size=0.1, random_state=1234, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "確認"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"len(train_datalist) =\", len(train_datalist))\n",
    "print(\"len(val_datalist) =\", len(val_datalist))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### データリスト関数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DatasetMaker(data.Dataset):\n",
    "    def __init__(self, datalist):\n",
    "        self.input_datalist = datalist.drop([\"PassengerId\", \"Survived\"], axis=1).values.astype(np.float32)\n",
    "        self.label_datalist = datalist[\"Survived\"].values.astype(np.int64)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_datalist)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        inputs = self.input_datalist[index]\n",
    "        labels = self.label_datalist[index]\n",
    "        return inputs, labels\n",
    "\n",
    "dataset = DatasetMaker(datalist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "確認"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"dataset.__len__() =\", dataset.__len__())\n",
    "print(\"dataset.__getitem__(index=0)[0] =\", dataset.__getitem__(index=0)[0])\n",
    "print(\"dataset.__getitem__(index=0)[1] =\", dataset.__getitem__(index=0)[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### データローダーの確認\n",
    " torch.utils.data.DataLoader()を呼び出すと使える"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 5\n",
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "batch_itr = iter(dataloader)\n",
    "inputs, labels = next(batch_itr)\n",
    "\n",
    "print(\"inputs =\\n\", inputs)\n",
    "print(\"inputs.size() =\", inputs.size())\n",
    "print(\"labels =\", labels)\n",
    "print(\"labels.size() =\", labels.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ネットワーククラスの定義"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "class Network(nn.Module):\n",
    "    def __init__(self, dim_inputs, dim_mid, dim_outputs, dropout_rate):\n",
    "        super().__init__()\n",
    "\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(dim_inputs, dim_mid),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=dropout_rate),\n",
    "            \n",
    "            nn.Linear(dim_mid, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=dropout_rate),\n",
    "            \n",
    "            nn.Linear(dim_mid, dim_outputs)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "net = Network(len(dataset.__getitem__(index=0)[0]), 64, 2, dropout_rate=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "確認"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(net)\n",
    "outputs = net(inputs)\n",
    "print(\"outputs = \\n\", outputs)\n",
    "print(\"outputs.size() =\", outputs.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trainerクラスを定義"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.optim as optim\n",
    "\n",
    "class Trainer:\n",
    "    def __init__(self, csv_path, num_epochs, batch_size, lr, save_weights_path):\n",
    "        # デバイスの設定（GPUが利用可能な場合はGPU、そうでない場合はCPUを使用）\n",
    "        self.device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "        print(\"self.device =\", self.device)\n",
    "\n",
    "        # トレーニングのパラメータを設定\n",
    "        self.num_epochs = num_epochs\n",
    "        self.save_weights_path = save_weights_path\n",
    "\n",
    "        # データの準備\n",
    "        datalist = makeDataList(csv_path)  # CSVファイルからデータリストを作成\n",
    "        # データを訓練用と検証用に分割（90%訓練、10%検証）\n",
    "        train_datalist, val_datalist = train_test_split(datalist, test_size=0.1, random_state=1234, shuffle=True)\n",
    "        \n",
    "        # データセットとデータローダーの作成\n",
    "        train_dataset = DatasetMaker(train_datalist)\n",
    "        val_dataset = DatasetMaker(val_datalist)\n",
    "        train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "        val_dataloader = torch.utils.data.DataLoader(val_dataset, batch_size=batch_size, shuffle=False, drop_last=False)\n",
    "        \n",
    "        # データローダーを辞書にまとめる\n",
    "        self.dataloaders_dict = {\"train\": train_dataloader, \"val\": val_dataloader}\n",
    "\n",
    "        # ネットワークモデルの初期化\n",
    "        self.net = Network(\n",
    "            dim_inputs = len(train_dataset.__getitem__(index=0)[0]),\n",
    "            dim_mid = 64,\n",
    "            dim_outputs = 2,\n",
    "            dropout_rate=0.1\n",
    "        )\n",
    "        self.net.to(self.device)  # モデルをGPUまたはCPUに移動\n",
    "\n",
    "        # 損失関数と最適化アルゴリズムの設定\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "        self.optimizer = optim.Adam(self.net.parameters(), lr=lr)\n",
    "\n",
    "\n",
    "    def train(self):\n",
    "\n",
    "        # 損失の記録用辞書\n",
    "        record_loss_dict = {\"train\": [], \"val\": []}\n",
    "        min_loss_epoch = 0.0\n",
    "\n",
    "        # エポックのループ\n",
    "        for epoch in range(self.num_epochs):\n",
    "            if epoch == 0 or not (epoch+1) % (self.num_epochs // 10):\n",
    "                print(\"----------\")\n",
    "                print(\"Epoch {}/{}\".format(epoch+1, self.num_epochs))\n",
    "\n",
    "            # 訓練フェーズと検証フェーズのループ\n",
    "            for phase in [\"train\", \"val\"]:\n",
    "                # モデルのモード設定（訓練時はtrain、検証時はeval）\n",
    "                if phase == \"train\":\n",
    "                    self.net.train()\n",
    "                else:\n",
    "                    self.net.eval()\n",
    "\n",
    "                # 損失と入力数の初期化\n",
    "                loss_epoch = 0.0\n",
    "                num_inputs = 0\n",
    "\n",
    "                # ミニバッチのループ\n",
    "                for inputs, labels in self.dataloaders_dict[phase]:\n",
    "                    inputs = inputs.to(self.device)\n",
    "                    labels = labels.to(self.device)\n",
    "\n",
    "                    # 勾配のリセット\n",
    "                    self.optimizer.zero_grad()\n",
    "\n",
    "                    # 訓練時のみ勾配を計算\n",
    "                    with torch.set_grad_enabled(phase == \"train\"):\n",
    "                        # 順伝播\n",
    "                        outputs = self.net(inputs)\n",
    "                        loss = self.criterion(outputs, labels)\n",
    "\n",
    "                        # 訓練時は逆伝播と最適化\n",
    "                        if phase == \"train\":\n",
    "                            loss.backward()\n",
    "                            self.optimizer.step()\n",
    "\n",
    "                    # 損失の累積\n",
    "                    loss_epoch += loss.item() * inputs.size(0)\n",
    "                    num_inputs += inputs.size(0)\n",
    "\n",
    "                # エポックごとの平均損失を計算\n",
    "                loss_epoch = loss_epoch / num_inputs\n",
    "                record_loss_dict[phase].append(loss_epoch)\n",
    "\n",
    "                if epoch == 0 or not (epoch+1) % (self.num_epochs // 10):\n",
    "                    print(\"{} Loss: {:.4f}\".format(phase, loss_epoch))\n",
    "\n",
    "            # 最良モデルの保存\n",
    "            if epoch == 0 or record_loss_dict[\"val\"][-1] < min_loss_epoch:\n",
    "                min_loss_epoch = record_loss_dict[\"val\"][-1]\n",
    "                torch.save(self.net.state_dict(), self.save_weights_path)\n",
    "\n",
    "        # 損失のグラフ表示\n",
    "        self.showGraph(record_loss_dict)\n",
    "\n",
    "    def showGraph(self, record_loss_dict):\n",
    "        # 訓練と検証の損失をグラフ化\n",
    "        graph = plt.figure()\n",
    "        plt.plot(range(len(record_loss_dict[\"train\"])), record_loss_dict[\"train\"], label=\"Training\")\n",
    "        plt.plot(range(len(record_loss_dict[\"val\"])), record_loss_dict[\"val\"], label=\"Validation\")\n",
    "        plt.legend()\n",
    "        plt.xlabel(\"Epoch\")\n",
    "        plt.ylabel(\"Loss\")\n",
    "        plt.title(\"last loss: train=\" + str(record_loss_dict[\"train\"][-1]) + \", val=\" + str(record_loss_dict[\"val\"][-1]))\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## モデルの訓練"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------\n",
      "Epoch 200/2000\n",
      "train Loss: 0.5186\n",
      "val Loss: 0.4571\n",
      "----------\n",
      "Epoch 400/2000\n",
      "train Loss: 0.4760\n",
      "val Loss: 0.4256\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 9\u001b[0m\n\u001b[1;32m      6\u001b[0m save_weights_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./weights.pth\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      8\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Trainer(csv_path, num_epochs, batch_size, lr, save_weights_path)\n\u001b[0;32m----> 9\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[24], line 77\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     74\u001b[0m labels \u001b[38;5;241m=\u001b[39m labels\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m     76\u001b[0m \u001b[38;5;66;03m# 勾配のリセット\u001b[39;00m\n\u001b[0;32m---> 77\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mzero_grad\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     79\u001b[0m \u001b[38;5;66;03m# 訓練時のみ勾配を計算\u001b[39;00m\n\u001b[1;32m     80\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(phase \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m     81\u001b[0m     \u001b[38;5;66;03m# 順伝播\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/_compile.py:24\u001b[0m, in \u001b[0;36m_disable_dynamo.<locals>.inner\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(fn)\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minner\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     22\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_dynamo\u001b[39;00m\n\u001b[0;32m---> 24\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dynamo\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdisable\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrecursive\u001b[49m\u001b[43m)\u001b[49m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/_dynamo/decorators.py:46\u001b[0m, in \u001b[0;36mdisable\u001b[0;34m(fn, recursive)\u001b[0m\n\u001b[1;32m     44\u001b[0m         fn \u001b[38;5;241m=\u001b[39m innermost_fn(fn)\n\u001b[1;32m     45\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(fn)\n\u001b[0;32m---> 46\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mDisableContext\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     47\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m DisableContext()\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:441\u001b[0m, in \u001b[0;36m_TorchDynamoContext.__call__\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    438\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m    439\u001b[0m     filename \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    440\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m--> 441\u001b[0m     (filename \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m \u001b[43mskipfiles\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcheck\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    442\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (\n\u001b[1;32m    443\u001b[0m         \u001b[38;5;28mgetattr\u001b[39m(fn, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__name__\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_call_impl\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_wrapped_call_impl\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    444\u001b[0m     )\n\u001b[1;32m    445\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m filename \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m DONT_WRAP_FILES\n\u001b[1;32m    446\u001b[0m ):\n\u001b[1;32m    447\u001b[0m     \u001b[38;5;66;03m# call to a builtin without a frame for us to capture\u001b[39;00m\n\u001b[1;32m    448\u001b[0m     fn \u001b[38;5;241m=\u001b[39m external_utils\u001b[38;5;241m.\u001b[39mwrap_inline(fn)\n\u001b[1;32m    450\u001b[0m callback \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/_dynamo/skipfiles.py:405\u001b[0m, in \u001b[0;36mcheck\u001b[0;34m(obj, is_inlined_call)\u001b[0m\n\u001b[1;32m    404\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcheck\u001b[39m(obj, is_inlined_call\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m--> 405\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcheck_verbose\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_inlined_call\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mskipped\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/_dynamo/skipfiles.py:389\u001b[0m, in \u001b[0;36mcheck_verbose\u001b[0;34m(obj, is_inlined_call)\u001b[0m\n\u001b[1;32m    387\u001b[0m     fi \u001b[38;5;241m=\u001b[39m FunctionInfo(obj, \u001b[38;5;28;01mNone\u001b[39;00m, getfile(obj), \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    388\u001b[0m \u001b[38;5;66;03m# Go through function based skip/inline rules.\u001b[39;00m\n\u001b[0;32m--> 389\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m fi\u001b[38;5;241m.\u001b[39mcode \u001b[38;5;129;01min\u001b[39;00m get_func_inlinelist():\n\u001b[1;32m    390\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m SkipResult(\n\u001b[1;32m    391\u001b[0m         \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    392\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minlined according skipfiles.FUNC_INLINELIST\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    393\u001b[0m     )\n\u001b[1;32m    394\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_inlined_call:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    csv_path = \"workspace/dataset/train.csv\"\n",
    "    num_epochs = 2000\n",
    "    batch_size = 80\n",
    "    lr = 0.0001\n",
    "    save_weights_path = \"./weights.pth\"\n",
    "\n",
    "    trainer = Trainer(csv_path, num_epochs, batch_size, lr, save_weights_path)\n",
    "    trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 課題内容\n",
    "\n",
    "## 1. コードの分離と再構築\n",
    "- `demo.ipynb`の内容を以下のように機能別に分離してください:\n",
    "  - `code/model.py`\n",
    "  - `code/train.py`\n",
    "  - `code/utils.py`\n",
    "- `train.py`を実行すると動作するように実装してください\n",
    "- 確認用のコードは不要です\n",
    "\n",
    "## 2. 推論の実装\n",
    "- 新しいファイル`code/predict.py`を作成してください\n",
    "- `dataset/test.csv`を入力データとして使用してください\n",
    "- 学習した重みをモデルで読み込んで推論を実行してください\n",
    "  - 注意: test.csvには`Survived`ラベルは含まれていないです\n",
    "- 推論結果を`result`フォルダ内にCSVファイルとして保存してください\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 課題\n",
    " #### ・demo.ipynbの中身を機能ごとにcodeフォルダの中のmodel.py,train.py,utils.pyに分離して、train.pyを実行すると動くようにしてみましょう\n",
    "  #### *確認部分はなくていいです\n",
    " #### ・datasetの中のtest.csvを入力データとして実際の推論を行ってみましょう \n",
    "　\n",
    "  ##### test.csvにはSurvivedのラベルは無いので、読み込んで学習したモデルに入力し、出力をcsvファイルとしてresultフォルダに保存してください"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyTorch",
   "language": "python",
   "name": "base"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
